{"title": "Using Small Language Models to Reverse-Engineer Machine Learning Pipelines Structures", "first_label": ["LLM"], "second_label": [], "data": "N Lacroix, M Blay-Fornarino, S Mosser, F Precioso- arXiv preprint arXiv:2601.03988, 2026\nBackground: Extracting the stages that structure Machine Learning (ML) pipelines \nfrom source code is key for gaining a deeper understanding of data science \npractices. However, the diversity caused by the constant evolution of the ML\n\u00a0\nGoogle Scholar gi thng bo ny cho bn v bn ang theo di nhng bi vit mi lin quan n nghin cu ca \nThanh Le-Cong\n.\nLit k cnh bo\nHy thng bo", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2601.03988&hl=vi&sa=X&d=7549434272671281564&ei=dURjaZX1PNrJieoPiYyysAk&scisig=AHkA5jTOGTsv2diCcUinMUqe6Upm&oi=scholaralrt&hist=70gU4M0AAAAJ:5337116523931328826:AHkA5jQSH2z-ynDQhPp_cW7HGs_g&html=&pos=0&folt=rel", "author": ["Thanh Le-Cong"], "ref": ["Thanh Le-Cong - nghi\u00ean c\u1ee9u li\u00ean quan m\u1edbi"]}
{"title": "Understanding the Role of Large Language Models in Software Engineering: Evidence from an Industry Survey", "first_label": ["LLM"], "second_label": [], "data": "VM de Brito, K Farias- arXiv preprint arXiv:2512.21347, 2025\nThe rapid advancement of Large Language Models (LLMs) is reshaping software \nengineering by profoundly influencing coding, documentation, and system \nmaintenance practices. As these tools become deeply embedded in developers'", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2512.21347&hl=vi&sa=X&d=5399442109481763722&ei=AfVhafWIJ8elieoP1s_TgA8&scisig=AHkA5jSprh5QZfsObVjcD27zCK-7&oi=scholaralrt&hist=70gU4M0AAAAJ:5337116523931328826:AHkA5jQSH2z-ynDQhPp_cW7HGs_g&html=&pos=0&folt=rel", "author": ["Thanh Le-Cong"], "ref": ["Thanh Le-Cong - nghi\u00ean c\u1ee9u li\u00ean quan m\u1edbi"]}
{"title": "FasterPy: An LLM-based Code Execution Efficiency Optimization Framework", "first_label": ["LLM", "Code"], "second_label": [], "data": "Y Wu, M Han, R Li, P Liang, A Tahir, Z Li, Q Feng- arXiv preprint arXiv, 2025\nCode often suffers from performance bugs. These bugs necessitate the research and \npractice of code optimization. Traditional rule-based methods rely on manually \ndesigning and maintaining rules for specific performance bugs (eg, redundant loops", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2512.22827&hl=vi&sa=X&d=11430556145799954815&ei=AfVhafWIJ8elieoP1s_TgA8&scisig=AHkA5jRpaKWh4y1Z8hftoa9d-vwY&oi=scholaralrt&hist=70gU4M0AAAAJ:5337116523931328826:AHkA5jQSH2z-ynDQhPp_cW7HGs_g&html=&pos=1&folt=rel", "author": ["Thanh Le-Cong"], "ref": ["Thanh Le-Cong - nghi\u00ean c\u1ee9u li\u00ean quan m\u1edbi"]}
{"title": "From User Interface to Agent Interface: Efficiency Optimization of UI Representations for LLM Agents", "first_label": ["LLM"], "second_label": ["Agent"], "data": "D Ran, Z Gong, Y Guo, M Wu, Y Cao, H Lu, H Zhang- arXiv preprint arXiv, 2025\nWhile Large Language Model (LLM) agents show great potential for automated UI \nnavigation such as automated UI testing and AI assistants, their efficiency has been \nlargely overlooked. Our motivating study reveals that inefficient UI representation", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2512.13438&hl=vi&sa=X&d=3194216199130525054&ei=AfVhafWIJ8elieoP1s_TgA8&scisig=AHkA5jS5j3LK7hUjhls8HAYC8CBM&oi=scholaralrt&hist=70gU4M0AAAAJ:5337116523931328826:AHkA5jQSH2z-ynDQhPp_cW7HGs_g&html=&pos=2&folt=rel", "author": ["Thanh Le-Cong"], "ref": ["Thanh Le-Cong - nghi\u00ean c\u1ee9u li\u00ean quan m\u1edbi"]}
{"title": "A Localization Framework for Reasoning Faults in LLM-based Code Agents", "first_label": ["LLM", "Code"], "second_label": ["Agent", "Localization", "Reasoning"], "data": "B Srivibhav, KS Reddy, NN Bhuvith, KES Ganesh - 2026\nAutonomous LLM-based code agents are rapidly advancing, yet their practical utility \nis hindered by a critical gap in our understanding of their failures. When an agent \nfails, the root cause is often not a simple code bug but a complex flaw in its internal\n\u00a0\nGoogle Scholar gi thng bo ny cho bn v bn ang theo di nhng bi vit mi lin quan n nghin cu ca \nThanh Le-Cong\n.\nLit k cnh bo\nHy thng bo", "link": "https://scholar.google.com/scholar_url?url=https://www.researchgate.net/profile/Shouvick-Mondal/publication/399514644_A_Localization_Framework_for_Reasoning_Faults_in_LLM-based_Code_Agents/links/695d67097e61d05b531795d3/A-Localization-Framework-for-Reasoning-Faults-in-LLM-based-Code-Agents.pdf&hl=vi&sa=X&d=11877855239271636437&ei=AfVhafWIJ8elieoP1s_TgA8&scisig=AHkA5jRAC2-sG24mCOJa9oorlNxb&oi=scholaralrt&hist=70gU4M0AAAAJ:5337116523931328826:AHkA5jQSH2z-ynDQhPp_cW7HGs_g&html=&pos=3&folt=rel", "author": ["Thanh Le-Cong"], "ref": ["Thanh Le-Cong - nghi\u00ean c\u1ee9u li\u00ean quan m\u1edbi"]}
{"title": "Machine learning for software engineering", "first_label": [], "second_label": [], "data": "S Balla - 2025\nThe explosive growth of open-source repositories creates opportunities and \nchallenges for Machine Learning for Software Engineering (ML4SE). Current \nmethods struggle with:(i) code frequently reformatted or minified, obscuring stylistic \nsignals;(ii) the lack of standardised benchmarks for repository recommendation; and \n(iii) the need to scale to billions of files in archives such as Software Heritage. \nObjectives. This thesis aims to (1) develop an authorship-attribution technique\nTrch dn: LEGION: Harnessing Pre-trained Language Models for GitHub\u00a0\u00a0\n\u00a0\nGoogle Scholar gi thng bo ny cho bn v bn ang theo di nhng li trch dn mi trong cc bi vit ca \nThanh Le-Cong\n.\nLit k cnh bo\nHy thng bo", "link": "https://scholar.google.com/scholar_url?url=https://amsdottorato.unibo.it/id/eprint/12493/1/Balla_Stefano_thesis.pdf&hl=vi&sa=X&d=1941060584359203803&ei=AfVhabyQNKyK6rQPgZOO6A8&scisig=AHkA5jS7H8GpOrye5VFEHOLrOi_x&oi=scholaralrt&hist=70gU4M0AAAAJ:6246953642887790424:AHkA5jTPS8thqNJxu8pHnPo4odW8&html=&pos=0&folt=cit", "author": ["Thanh Le-Cong"], "ref": ["1 l\u1eddi tr\u00edch d\u1eabn m\u1edbi \u0111\u1ebfn b\u00e0i vi\u1ebft c\u1ee7a Thanh Le-Cong"]}
