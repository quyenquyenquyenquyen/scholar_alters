{"title": "CATCODER: Repository-Level Code Generation with Relevant Code and Type Context", "first_label": ["Code", "Repository-Level"], "second_label": ["Generation"], "data": "Z Pan, X Hu, X Xia, X Yang- ACM Transactions on Software Engineering and, 2025\nLarge language models (LLMs) have demonstrated remarkable capabilities in code \ngeneration tasks. However, repository-level code generation presents unique \nchallenges, particularly due to the need to utilize information spread across multiple", "link": "https://scholar.google.com/scholar_url?url=https://dl.acm.org/doi/pdf/10.1145/3779217&hl=vi&sa=X&d=11385254115962353809&ei=XoBFaeSPFK-nieoPz8HDqAc&scisig=ALhkC2TN2vBgsLrQYPp-W0a5NsX_&oi=scholaralrt&hist=70gU4M0AAAAJ:5337116523931328826:ALhkC2T2gGOcVPTEcMFcARghNUJN&html=&pos=0&folt=rel", "author": ["Thanh Le-Cong"], "ref": ["Thanh Le-Cong - nghi\u00ean c\u1ee9u li\u00ean quan m\u1edbi"]}
{"title": "Scaling Laws for Code: Every Programming Language Matters", "first_label": ["Code"], "second_label": [], "data": "J Yang, S Guo, L Jing, W Zhang, A Liu, C Hao, Z Li- arXiv preprint arXiv, 2025\nCode large language models (Code LLMs) are powerful but costly to train, with \nscaling laws predicting performance from model size, data, and compute. However, \ndifferent programming languages (PLs) have varying impacts during pre-training that", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2512.13472&hl=vi&sa=X&d=14773747412183223538&ei=XoBFaeSPFK-nieoPz8HDqAc&scisig=ALhkC2R6TNZd4FCNSpWIuNbe9QIF&oi=scholaralrt&hist=70gU4M0AAAAJ:5337116523931328826:ALhkC2T2gGOcVPTEcMFcARghNUJN&html=&pos=1&folt=rel", "author": ["Thanh Le-Cong"], "ref": ["Thanh Le-Cong - nghi\u00ean c\u1ee9u li\u00ean quan m\u1edbi"]}
{"title": "LLM Assisted Coding with Metamorphic Specification Mutation Agent", "first_label": ["LLM"], "second_label": ["Agent"], "data": "MR Akhond, G Uddin- arXiv preprint arXiv:2511.18249, 2025\nMetamorphic Relations (MRs) serve as a foundational mechanism for generating \nsemantically equivalent mutations. Software engineering has advanced significantly \nin recent years with the advent of Large Language Models (LLMs). However, the", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2511.18249&hl=vi&sa=X&d=10413103399697962147&ei=XoBFaeSPFK-nieoPz8HDqAc&scisig=ALhkC2TLO2INEEpsMtcyeSceXBK6&oi=scholaralrt&hist=70gU4M0AAAAJ:5337116523931328826:ALhkC2T2gGOcVPTEcMFcARghNUJN&html=&pos=2&folt=rel", "author": ["Thanh Le-Cong"], "ref": ["Thanh Le-Cong - nghi\u00ean c\u1ee9u li\u00ean quan m\u1edbi"]}
{"title": "Adapting Language Models for Low-Resource Programming Languages", "first_label": ["LLM"], "second_label": [], "data": "A Singha, M Singh, H Hasanbeig, A Radhakrishna- NeurIPS 2025 Fourth Workshop on\nLarge Language Models (LLMs) have achieved remarkable success in code \ngeneration, yet their capabilities remain predominantly concentrated in well-\nresourced programming languages such as Python and Java. In contrast, low", "link": "https://scholar.google.com/scholar_url?url=https://openreview.net/pdf%3Fid%3D2ctRK8h3AZ&hl=vi&sa=X&d=10639384588679978541&ei=XoBFaeSPFK-nieoPz8HDqAc&scisig=ALhkC2TU1E8-xJojBcX0Q0YHF0cb&oi=scholaralrt&hist=70gU4M0AAAAJ:5337116523931328826:ALhkC2T2gGOcVPTEcMFcARghNUJN&html=&pos=3&folt=rel", "author": ["Thanh Le-Cong"], "ref": ["Thanh Le-Cong - nghi\u00ean c\u1ee9u li\u00ean quan m\u1edbi"]}
{"title": "Evaluating and improving LLM-based competitive program generation", "first_label": ["LLM"], "second_label": ["Generation"], "data": "M Wei, Z Li, X Chen, M Zheng, Z Qu, C Yu, S Chen- Information and Software, 2025\nContext: Due to the demand for strong algorithmic reasoning, complex logic \nimplementation, and strict adherence to input/output formats and resource \nconstraints, competitive programming generation by large language models (LLMs)", "link": "https://scholar.google.com/scholar_url?url=https://www.sciencedirect.com/science/article/pii/S0950584925003167&hl=vi&sa=X&d=5709124783036459514&ei=XoBFaeSPFK-nieoPz8HDqAc&scisig=ALhkC2RbIKQLHP2C9FQAkypoxHIi&oi=scholaralrt&hist=70gU4M0AAAAJ:5337116523931328826:ALhkC2T2gGOcVPTEcMFcARghNUJN&html=&pos=4&folt=rel", "author": ["Thanh Le-Cong"], "ref": ["Thanh Le-Cong - nghi\u00ean c\u1ee9u li\u00ean quan m\u1edbi"]}
{"title": "Instruction-Tuning Open-Weight Language Models for BPMN Model Generation", "first_label": ["LLM"], "second_label": ["Generation"], "data": "G elikmasat, A zgvde, FB Aydemir- arXiv preprint arXiv:2512.12063, 2025\nDomain models are central to software engineering, as they enable a shared \nunderstanding, guide implementation, and support automated analyses and model-\ndriven development. Yet, despite these benefits, practitioners often skip modeling", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2512.12063&hl=vi&sa=X&d=3990484517725756495&ei=XoBFaeSPFK-nieoPz8HDqAc&scisig=ALhkC2TElHBGgrhYPeHZw5tz7kVf&oi=scholaralrt&hist=70gU4M0AAAAJ:5337116523931328826:ALhkC2T2gGOcVPTEcMFcARghNUJN&html=&pos=5&folt=rel", "author": ["Thanh Le-Cong"], "ref": ["Thanh Le-Cong - nghi\u00ean c\u1ee9u li\u00ean quan m\u1edbi"]}
{"title": "The price of precision: the cost of preprocessing for automated code revision in code review", "first_label": ["Code Review", "Code"], "second_label": [], "data": "S Pirouzkhah, P Rani, F Sovrano, V Hellendoorn- Empirical Software, 2026\nCode review is a widespread practice in software engineering during which \ndevelopers examine each other's source code changes to identify potential issues \nand improve code quality. Among the automated techniques proposed by\n\u00a0\nGoogle Scholar gi thng bo ny cho bn v bn ang theo di nhng bi vit mi lin quan n nghin cu ca \nThanh Le-Cong\n.\nLit k cnh bo\nHy thng bo", "link": "https://scholar.google.com/scholar_url?url=https://link.springer.com/article/10.1007/s10664-025-10781-4&hl=vi&sa=X&d=13774619557062862958&ei=XoBFaeSPFK-nieoPz8HDqAc&scisig=ALhkC2SQ9XaojmC36xeZ2hsyNU3U&oi=scholaralrt&hist=70gU4M0AAAAJ:5337116523931328826:ALhkC2T2gGOcVPTEcMFcARghNUJN&html=&pos=6&folt=rel", "author": ["Thanh Le-Cong"], "ref": ["Thanh Le-Cong - nghi\u00ean c\u1ee9u li\u00ean quan m\u1edbi"]}
{"title": "A data-augmented model routing framework for efficient LLM deployment in edgecloud environments: MSM Pozi, Y. Sato", "first_label": ["LLM"], "second_label": [], "data": "MSM Pozi, Y Sato- The Journal of Supercomputing, 2025\nLarge language model (LLM)-based program generation tasks are hindered by high \ncomputational demands. These challenges, along with high deployment costs, often \npose a barrier to practical applications. To address these, we propose a novel data", "link": "https://scholar.google.com/scholar_url?url=https://link.springer.com/article/10.1007/s11227-025-08034-8&hl=vi&sa=X&d=3548574653858656275&ei=iuBDaaLMCd_OieoPg6WE2QY&scisig=ALhkC2Q4-VJc0taO10AjW_V1NOD2&oi=scholaralrt&hist=70gU4M0AAAAJ:5337116523931328826:ALhkC2T2gGOcVPTEcMFcARghNUJN&html=&pos=0&folt=rel", "author": ["Thanh Le-Cong"], "ref": ["Thanh Le-Cong - nghi\u00ean c\u1ee9u li\u00ean quan m\u1edbi"]}
{"title": "KTester: Leveraging Domain and Testing Knowledge for More Effective LLM-based Test Generation", "first_label": ["LLM", "Software Testing"], "second_label": ["Generation"], "data": "A Li, M Liu, Z Chen, Z Pei, Z Li, D Dai, Y Wang, Z Zheng- arXiv preprint arXiv, 2025\nAutomated unit test generation using large language models (LLMs) holds great \npromise but often struggles with generating tests that are both correct and \nmaintainable in real-world projects. This paper presents KTester, a novel framework", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2511.14224&hl=vi&sa=X&d=10983934054869938261&ei=iuBDaaLMCd_OieoPg6WE2QY&scisig=ALhkC2SnqeAXUhGFyjUYbpo2ILAS&oi=scholaralrt&hist=70gU4M0AAAAJ:5337116523931328826:ALhkC2T2gGOcVPTEcMFcARghNUJN&html=&pos=1&folt=rel", "author": ["Thanh Le-Cong"], "ref": ["Thanh Le-Cong - nghi\u00ean c\u1ee9u li\u00ean quan m\u1edbi"]}
{"title": "CIV: Leveraging Causal Subgraphs of Vulnerability for Noise Reduction in Vulnerability Detection", "first_label": ["Vulnerabilities"], "second_label": ["Detection", "Graph"], "data": "Z Gao, L Xiao, X Du, Y Xing- Expert Systems with Applications, 2025\nAccurate vulnerability detection is critical for software security. Although deep \nlearning-based vulnerability detection methods have shown promise in this task, they \ninclude much information unrelated to vulnerability semantics, which we call noise", "link": "https://scholar.google.com/scholar_url?url=https://www.sciencedirect.com/science/article/pii/S0957417425040072&hl=vi&sa=X&d=1911796227503386901&ei=iuBDaaLMCd_OieoPg6WE2QY&scisig=ALhkC2RYTrdtkri1T9-q3e3-4G2W&oi=scholaralrt&hist=70gU4M0AAAAJ:5337116523931328826:ALhkC2T2gGOcVPTEcMFcARghNUJN&html=&pos=2&folt=rel", "author": ["Thanh Le-Cong"], "ref": ["Thanh Le-Cong - nghi\u00ean c\u1ee9u li\u00ean quan m\u1edbi"]}
{"title": "The Cost of AI-Assisted Coding: Energy vs. Accuracy in Language Models", "first_label": ["LLM"], "second_label": [], "data": "N Alizadeh, B Belchev, N Saurabh, P Kelbert - 2025\nGenerative Large Language Models (LLMs) have become widely accessible since \nthe release of ChatGPT in late 2022 [2], and their adoption nearly doubled in under \nsix months [3]. In addition, the majority of developers find code-specific AI models", "link": "https://scholar.google.com/scholar_url?url=https://benevol2025.github.io/pre/paper03.pdf&hl=vi&sa=X&d=3874342652444288509&ei=iuBDaaLMCd_OieoPg6WE2QY&scisig=ALhkC2Rn562zMHnFoj6JiddHdWsr&oi=scholaralrt&hist=70gU4M0AAAAJ:5337116523931328826:ALhkC2T2gGOcVPTEcMFcARghNUJN&html=&pos=3&folt=rel", "author": ["Thanh Le-Cong"], "ref": ["Thanh Le-Cong - nghi\u00ean c\u1ee9u li\u00ean quan m\u1edbi"]}
{"title": "CrossPyEval: Enhancing LLM-based Evaluation of Low-Resource Code via Code Translation", "first_label": ["LLM", "Code"], "second_label": ["Generation"], "data": "W Wu, LI Wu, G Li- The 17th Asian Conference on Machine Learning, 2025\nLarge language models (LLMs) have demonstrated remarkable performance in code \ngeneration and evaluation tasks, particularly for Python, which dominates the pre-\ntraining corpora. However, the evaluation of code in low-resource programming\n\u00a0\nGoogle Scholar gi thng bo ny cho bn v bn ang theo di nhng bi vit mi lin quan n nghin cu ca \nThanh Le-Cong\n.\nLit k cnh bo\nHy thng bo", "link": "https://scholar.google.com/scholar_url?url=https://openreview.net/pdf%3Fid%3D5qiFpW4lWH&hl=vi&sa=X&d=15885618439171632205&ei=iuBDaaLMCd_OieoPg6WE2QY&scisig=ALhkC2SIGNUB0bs_75fnhmkDha8R&oi=scholaralrt&hist=70gU4M0AAAAJ:5337116523931328826:ALhkC2T2gGOcVPTEcMFcARghNUJN&html=&pos=4&folt=rel", "author": ["Thanh Le-Cong"], "ref": ["Thanh Le-Cong - nghi\u00ean c\u1ee9u li\u00ean quan m\u1edbi"]}
