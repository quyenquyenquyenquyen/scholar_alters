{"title": "Chasing Shadows: Pitfalls in LLM Security Research", "first_label": ["LLM"], "second_label": ["Search"], "data": "J Evertz, N Risse, N Neuer, A Mller, P Normann- arXiv preprint arXiv, 2025\nLarge language models (LLMs) are increasingly prevalent in security research. Their \nunique characteristics, however, introduce challenges that undermine established \nparadigms of reproducibility, rigor, and evaluation. Prior work has identified common", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2512.09549&hl=vi&sa=X&d=293134883374188418&ei=pO8-aciyJpiTieoP5LbfuQw&scisig=ALhkC2ReCa7-FbIPRRuJnuVr3o7l&oi=scholaralrt&hist=70gU4M0AAAAJ:5337116523931328826:ALhkC2T2gGOcVPTEcMFcARghNUJN&html=&pos=0&folt=rel", "author": ["Thanh Le-Cong"], "ref": ["Thanh Le-Cong - nghi\u00ean c\u1ee9u li\u00ean quan m\u1edbi"]}
{"title": "BugSweeper: Function-Level Detection of Smart Contract Vulnerabilities Using Graph Neural Networks", "first_label": ["Vulnerabilities", "Smart Contracts", "Bug"], "second_label": ["Detection", "Graph"], "data": "U Lee, C Chung, J Lee, SM Moon- arXiv preprint arXiv:2512.09385, 2025\nThe rapid growth of Ethereum has made it more important to quickly and accurately \ndetect smart contract vulnerabilities. While machine-learning-based methods have \nshown some promise, many still rely on rule-based preprocessing designed by", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2512.09385&hl=vi&sa=X&d=13481494175582653042&ei=pO8-aciyJpiTieoP5LbfuQw&scisig=ALhkC2SSJQ1OzR4VoQZBqgOV7Eju&oi=scholaralrt&hist=70gU4M0AAAAJ:5337116523931328826:ALhkC2T2gGOcVPTEcMFcARghNUJN&html=&pos=1&folt=rel", "author": ["Thanh Le-Cong"], "ref": ["Thanh Le-Cong - nghi\u00ean c\u1ee9u li\u00ean quan m\u1edbi"]}
{"title": "Adapting Language Models for Low-Resource Programming Languages", "first_label": ["LLM"], "second_label": [], "data": "A Singha, M Singh, H Hasanbeig, A Radhakrishna- NeurIPS 2025 Fourth Workshop on\nLarge Language Models (LLMs) have achieved remarkable success in code \ngeneration, yet their capabilities remain predominantly concentrated in well-\nresourced programming languages such as Python and Java. In contrast, low\n\u00a0\nGoogle Scholar gi thng bo ny cho bn v bn ang theo di nhng bi vit mi lin quan n nghin cu ca \nThanh Le-Cong\n.\nLit k cnh bo\nHy thng bo", "link": "https://scholar.google.com/scholar_url?url=https://openreview.net/pdf%3Fid%3D2ctRK8h3AZ&hl=vi&sa=X&d=10639384588679978541&ei=pO8-aciyJpiTieoP5LbfuQw&scisig=ALhkC2TU1E8-xJojBcX0Q0YHF0cb&oi=scholaralrt&hist=70gU4M0AAAAJ:5337116523931328826:ALhkC2T2gGOcVPTEcMFcARghNUJN&html=&pos=2&folt=rel", "author": ["Thanh Le-Cong"], "ref": ["Thanh Le-Cong - nghi\u00ean c\u1ee9u li\u00ean quan m\u1edbi"]}
{"title": "Large Language Models for Education and Research: An Empirical and User Survey-based Analysis", "first_label": ["LLM"], "second_label": ["Search"], "data": "MM Rahman, AI Shiplu, MFI Amin, Y Watanobe, L Peng- arXiv preprint arXiv, 2025\nPretrained Large Language Models (LLMs) have achieved remarkable success \nacross diverse domains, with education and research emerging as particularly \nimpactful areas. Among current state-of-the-art LLMs, ChatGPT and DeepSeek \nexhibit strong capabilities in mathematics, science, medicine, literature, and \nprogramming. In this study, we present a comprehensive evaluation of these two \nLLMs through background technology analysis, empirical experiments, and a real\nTrch dn: Refining chatgpt-generated code: Characterizing and mitigating\u00a0\u00a0\n\u00a0\nGoogle Scholar gi thng bo ny cho bn v bn ang theo di nhng li trch dn mi trong cc bi vit ca \nThanh Le-Cong\n.\nLit k cnh bo\nHy thng bo", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2512.08057&hl=vi&sa=X&d=1651554278479972452&ei=s4w9abL1KMSN6rQPx4HkwQU&scisig=ALhkC2Rv1zkjbxoeB4xprGaQRv8r&oi=scholaralrt&hist=70gU4M0AAAAJ:6246953642887790424:ALhkC2S5kr7kHUQq0DGLL-ZCgBVB&html=&pos=0&folt=cit", "author": ["Thanh Le-Cong"], "ref": ["1 l\u1eddi tr\u00edch d\u1eabn m\u1edbi \u0111\u1ebfn b\u00e0i vi\u1ebft c\u1ee7a Thanh Le-Cong"]}
{"title": "LoCoBench-Agent: An Interactive Benchmark for LLM Agents in Long-Context Software Engineering", "first_label": ["LLM"], "second_label": ["Agent"], "data": "J Qiu, Z Liu, Z Liu, R Murthy, J Zhang, H Chen, S Wang- arXiv preprint arXiv, 2025\nAs large language models (LLMs) evolve into sophisticated autonomous agents \ncapable of complex software development tasks, evaluating their real-world \ncapabilities becomes critical. While existing benchmarks like LoCoBench~\\cite", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2511.13998&hl=vi&sa=X&d=5068327394060475494&ei=s4w9aZ7OHfO16rQPzoDK8Ag&scisig=ALhkC2RovNoV0ywYVr4A68T8Kt-V&oi=scholaralrt&hist=70gU4M0AAAAJ:5337116523931328826:ALhkC2T2gGOcVPTEcMFcARghNUJN&html=&pos=0&folt=rel", "author": ["Thanh Le-Cong"], "ref": ["Thanh Le-Cong - nghi\u00ean c\u1ee9u li\u00ean quan m\u1edbi"]}
{"title": "A data-augmented model routing framework for efficient LLM deployment in edgecloud environments: MSM Pozi, Y. Sato", "first_label": ["LLM"], "second_label": [], "data": "MSM Pozi, Y Sato- The Journal of Supercomputing, 2025\nLarge language model (LLM)-based program generation tasks are hindered by high \ncomputational demands. These challenges, along with high deployment costs, often \npose a barrier to practical applications. To address these, we propose a novel data", "link": "https://scholar.google.com/scholar_url?url=https://link.springer.com/article/10.1007/s11227-025-08034-8&hl=vi&sa=X&d=3548574653858656275&ei=s4w9aZ7OHfO16rQPzoDK8Ag&scisig=ALhkC2Q4-VJc0taO10AjW_V1NOD2&oi=scholaralrt&hist=70gU4M0AAAAJ:5337116523931328826:ALhkC2T2gGOcVPTEcMFcARghNUJN&html=&pos=1&folt=rel", "author": ["Thanh Le-Cong"], "ref": ["Thanh Le-Cong - nghi\u00ean c\u1ee9u li\u00ean quan m\u1edbi"]}
{"title": "Multicalibration for LLM-based Code Generation", "first_label": ["LLM", "Code"], "second_label": ["Generation"], "data": "V Campos, R Kuschnereit, A Ulges- arXiv preprint arXiv:2512.08810, 2025\nAs AI-based code generation becomes widespread, researchers are investigating \nthe calibration of code LLMs-ensuring their confidence scores faithfully represent the \ntrue likelihood of code correctness. To do so, we investigate multicalibration, which", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2512.08810&hl=vi&sa=X&d=72436777158966715&ei=s4w9aZ7OHfO16rQPzoDK8Ag&scisig=ALhkC2RlDnVOjLzmpVcYzD3Jg4Fm&oi=scholaralrt&hist=70gU4M0AAAAJ:5337116523931328826:ALhkC2T2gGOcVPTEcMFcARghNUJN&html=&pos=2&folt=rel", "author": ["Thanh Le-Cong"], "ref": ["Thanh Le-Cong - nghi\u00ean c\u1ee9u li\u00ean quan m\u1edbi"]}
{"title": "LLM Assisted Coding with Metamorphic Specification Mutation Agent", "first_label": ["LLM"], "second_label": ["Agent"], "data": "MR Akhond, G Uddin- arXiv preprint arXiv:2511.18249, 2025\nMetamorphic Relations (MRs) serve as a foundational mechanism for generating \nsemantically equivalent mutations. Software engineering has advanced significantly \nin recent years with the advent of Large Language Models (LLMs). However, the", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2511.18249&hl=vi&sa=X&d=10413103399697962147&ei=s4w9aZ7OHfO16rQPzoDK8Ag&scisig=ALhkC2TLO2INEEpsMtcyeSceXBK6&oi=scholaralrt&hist=70gU4M0AAAAJ:5337116523931328826:ALhkC2T2gGOcVPTEcMFcARghNUJN&html=&pos=3&folt=rel", "author": ["Thanh Le-Cong"], "ref": ["Thanh Le-Cong - nghi\u00ean c\u1ee9u li\u00ean quan m\u1edbi"]}
{"title": "KTester: Leveraging Domain and Testing Knowledge for More Effective LLM-based Test Generation", "first_label": ["LLM", "Software Testing"], "second_label": ["Generation"], "data": "A Li, M Liu, Z Chen, Z Pei, Z Li, D Dai, Y Wang, Z Zheng- arXiv preprint arXiv, 2025\nAutomated unit test generation using large language models (LLMs) holds great \npromise but often struggles with generating tests that are both correct and \nmaintainable in real-world projects. This paper presents KTester, a novel framework", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2511.14224&hl=vi&sa=X&d=10983934054869938261&ei=s4w9aZ7OHfO16rQPzoDK8Ag&scisig=ALhkC2SnqeAXUhGFyjUYbpo2ILAS&oi=scholaralrt&hist=70gU4M0AAAAJ:5337116523931328826:ALhkC2T2gGOcVPTEcMFcARghNUJN&html=&pos=4&folt=rel", "author": ["Thanh Le-Cong"], "ref": ["Thanh Le-Cong - nghi\u00ean c\u1ee9u li\u00ean quan m\u1edbi"]}
{"title": "CIV: Leveraging Causal Subgraphs of Vulnerability for Noise Reduction in Vulnerability Detection", "first_label": ["Vulnerabilities"], "second_label": ["Detection", "Graph"], "data": "Z Gao, L Xiao, X Du, Y Xing- Expert Systems with Applications, 2025\nAccurate vulnerability detection is critical for software security. Although deep \nlearning-based vulnerability detection methods have shown promise in this task, they \ninclude much information unrelated to vulnerability semantics, which we call noise", "link": "https://scholar.google.com/scholar_url?url=https://www.sciencedirect.com/science/article/pii/S0957417425040072&hl=vi&sa=X&d=1911796227503386901&ei=s4w9aZ7OHfO16rQPzoDK8Ag&scisig=ALhkC2RYTrdtkri1T9-q3e3-4G2W&oi=scholaralrt&hist=70gU4M0AAAAJ:5337116523931328826:ALhkC2T2gGOcVPTEcMFcARghNUJN&html=&pos=5&folt=rel", "author": ["Thanh Le-Cong"], "ref": ["Thanh Le-Cong - nghi\u00ean c\u1ee9u li\u00ean quan m\u1edbi"]}
{"title": "Go-UT-Bench: A Fine-Tuning Dataset for LLM-Based Unit Test Generation in Go", "first_label": ["LLM", "Software Testing"], "second_label": ["Generation"], "data": "Y Pipalani, H Raj, R Ghosh, V Bhargava, D Dutta- arXiv preprint arXiv:2511.10868, 2025\nTraining data imbalance poses a major challenge for code LLMs. Most available \ndata heavily over represents raw opensource code while underrepresenting broader \nsoftware engineering tasks, especially in low resource languages like Golang. As a", "link": "https://scholar.google.com/scholar_url?url=https://arxiv.org/pdf/2511.10868&hl=vi&sa=X&d=16451546716691370203&ei=s4w9aZ7OHfO16rQPzoDK8Ag&scisig=ALhkC2S7c4klrU0tkIMOQ5Ni8xrY&oi=scholaralrt&hist=70gU4M0AAAAJ:5337116523931328826:ALhkC2T2gGOcVPTEcMFcARghNUJN&html=&pos=6&folt=rel", "author": ["Thanh Le-Cong"], "ref": ["Thanh Le-Cong - nghi\u00ean c\u1ee9u li\u00ean quan m\u1edbi"]}
{"title": "The Cost of AI-Assisted Coding: Energy vs. Accuracy in Language Models", "first_label": ["LLM"], "second_label": [], "data": "N Alizadeh, B Belchev, N Saurabh, P Kelbert - 2025\nGenerative Large Language Models (LLMs) have become widely accessible since \nthe release of ChatGPT in late 2022 [2], and their adoption nearly doubled in under \nsix months [3]. In addition, the majority of developers find code-specific AI models", "link": "https://scholar.google.com/scholar_url?url=https://benevol2025.github.io/pre/paper03.pdf&hl=vi&sa=X&d=3874342652444288509&ei=s4w9aZ7OHfO16rQPzoDK8Ag&scisig=ALhkC2Rn562zMHnFoj6JiddHdWsr&oi=scholaralrt&hist=70gU4M0AAAAJ:5337116523931328826:ALhkC2T2gGOcVPTEcMFcARghNUJN&html=&pos=7&folt=rel", "author": ["Thanh Le-Cong"], "ref": ["Thanh Le-Cong - nghi\u00ean c\u1ee9u li\u00ean quan m\u1edbi"]}
{"title": "CrossPyEval: Enhancing LLM-based Evaluation of Low-Resource Code via Code Translation", "first_label": ["LLM", "Code"], "second_label": ["Generation"], "data": "W Wu, LI Wu, G Li- The 17th Asian Conference on Machine Learning, 2025\nLarge language models (LLMs) have demonstrated remarkable performance in code \ngeneration and evaluation tasks, particularly for Python, which dominates the pre-\ntraining corpora. However, the evaluation of code in low-resource programming\n\u00a0\nGoogle Scholar gi thng bo ny cho bn v bn ang theo di nhng bi vit mi lin quan n nghin cu ca \nThanh Le-Cong\n.\nLit k cnh bo\nHy thng bo", "link": "https://scholar.google.com/scholar_url?url=https://openreview.net/pdf%3Fid%3D5qiFpW4lWH&hl=vi&sa=X&d=15885618439171632205&ei=s4w9aZ7OHfO16rQPzoDK8Ag&scisig=ALhkC2SIGNUB0bs_75fnhmkDha8R&oi=scholaralrt&hist=70gU4M0AAAAJ:5337116523931328826:ALhkC2T2gGOcVPTEcMFcARghNUJN&html=&pos=8&folt=rel", "author": ["Thanh Le-Cong"], "ref": ["Thanh Le-Cong - nghi\u00ean c\u1ee9u li\u00ean quan m\u1edbi"]}
